{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:03:38.839345Z","iopub.status.busy":"2023-11-02T16:03:38.838397Z","iopub.status.idle":"2023-11-02T16:03:51.374956Z","shell.execute_reply":"2023-11-02T16:03:51.373749Z","shell.execute_reply.started":"2023-11-02T16:03:38.839300Z"},"trusted":true},"outputs":[],"source":["!pip install captum"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:04:13.000312Z","iopub.status.busy":"2023-11-02T16:04:12.999897Z","iopub.status.idle":"2023-11-02T16:04:13.077579Z","shell.execute_reply":"2023-11-02T16:04:13.076581Z","shell.execute_reply.started":"2023-11-02T16:04:13.000272Z"},"trusted":true},"outputs":[],"source":["# pytorch組件\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.io import read_image\n","\n","# 基礎套件\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import numpy as np\n","import random\n","from collections import Counter\n","\n","# sklearn套包\n","from sklearn.metrics import recall_score, f1_score, classification_report, accuracy_score\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import confusion_matrix\n","from PIL import Image\n","\n","# 訓練讀調\n","from tqdm import tqdm, trange\n","\n","# 要train很多次的東西\n","import optuna\n","\n","# 可視化AI\n","from captum.attr import IntegratedGradients"]},{"cell_type":"markdown","metadata":{},"source":["## **設定GPU模式與決定隨機**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:13:25.721623Z","iopub.status.busy":"2023-11-02T14:13:25.721233Z","iopub.status.idle":"2023-11-02T14:13:25.728800Z","shell.execute_reply":"2023-11-02T14:13:25.727772Z","shell.execute_reply.started":"2023-11-02T14:13:25.721591Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda:0')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:57:04.741561Z","iopub.status.busy":"2023-11-02T14:57:04.741189Z","iopub.status.idle":"2023-11-02T14:57:04.749483Z","shell.execute_reply":"2023-11-02T14:57:04.748521Z","shell.execute_reply.started":"2023-11-02T14:57:04.741531Z"},"trusted":true},"outputs":[],"source":["# 決定Seed\n","# seed = 695\n","# torch.manual_seed(seed)\n","# torch.cuda.manual_seed(seed)\n","# torch.cuda.manual_seed_all(seed)\n","# np.random.seed(seed)\n","# random.seed(seed)\n","# torch.backends.cudnn.benchmark = False\n","# torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{},"source":["## **資料前處理**\n","1. 從各類別讀取所有資料並給與label\n","2. 合併各類資料成一個Dataframe\n","3. 把不要的去除掉"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:16:26.466807Z","iopub.status.busy":"2023-11-02T14:16:26.465952Z","iopub.status.idle":"2023-11-02T14:16:26.497945Z","shell.execute_reply":"2023-11-02T14:16:26.496966Z","shell.execute_reply.started":"2023-11-02T14:16:26.466761Z"},"trusted":true},"outputs":[],"source":["# 健康設為1\n","health_list = [] #各筆資料連結\n","health_label = [] #各筆資料label\n","for dirname, file, filename in os.walk(r'/kaggle/input/syn-time-series/region_health'):\n","    \n","    if len(filename) > 0:\n","        \n","        for files in filename:                \n","            \n","            health_list.append(os.path.join(dirname, files)) \n","            health_label.append(1)\n","         \n","print(len(health_list))\n","health_list[:10], health_label[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:16:44.051998Z","iopub.status.busy":"2023-11-02T14:16:44.051223Z","iopub.status.idle":"2023-11-02T14:16:44.073737Z","shell.execute_reply":"2023-11-02T14:16:44.072828Z","shell.execute_reply.started":"2023-11-02T14:16:44.051964Z"},"trusted":true},"outputs":[],"source":["# unhealth\n","unhealth_list = []\n","unhealth_label = []\n","\n","for dirname, file, filename in os.walk(r'/kaggle/input/syn-time-series/region_unhealth'):\n","    \n","    if len(filename) > 0:\n","        \n","        for files in filename:                \n","            \n","            unhealth_list.append(os.path.join(dirname, files))\n","            unhealth_label.append(0)        \n","\n","print(len(unhealth_list))\n","unhealth_list[20:30], unhealth_label[20:30]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:17:18.711766Z","iopub.status.busy":"2023-11-02T14:17:18.711391Z","iopub.status.idle":"2023-11-02T14:17:18.724575Z","shell.execute_reply":"2023-11-02T14:17:18.723477Z","shell.execute_reply.started":"2023-11-02T14:17:18.711736Z"},"trusted":true},"outputs":[],"source":["# 加再一起，還沒打亂\n","all_list = health_list + unhealth_list \n","all_label = health_label + unhealth_label\n","\n","# 建成dataframe\n","df_kaggle = pd.DataFrame()\n","df_kaggle['picture'] = all_list\n","df_kaggle['label'] = all_label\n","df_kaggle.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:52:40.941295Z","iopub.status.busy":"2023-11-02T14:52:40.940777Z","iopub.status.idle":"2023-11-02T14:52:40.955722Z","shell.execute_reply":"2023-11-02T14:52:40.954795Z","shell.execute_reply.started":"2023-11-02T14:52:40.941258Z"},"trusted":true},"outputs":[],"source":["# 刪除不要的data\n","df_second = df_kaggle[df_kaggle['picture'] != '/kaggle/input/syn-time-series/region_health/INT-033_HBA_Probe1_Oxy_region.npy']\n","df_second.index = range(len(df_second))\n","df_second"]},{"cell_type":"markdown","metadata":{},"source":["## **製作訓練資料**\n","1. 使用StratifiedKFold分訓練以及測試資料\n","2. 建立dataset\n","3. 放入dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:52:41.744155Z","iopub.status.busy":"2023-11-02T14:52:41.743750Z","iopub.status.idle":"2023-11-02T14:52:41.751217Z","shell.execute_reply":"2023-11-02T14:52:41.750090Z","shell.execute_reply.started":"2023-11-02T14:52:41.744127Z"},"trusted":true},"outputs":[],"source":["# dataload 用dataset去inheret\n","class CustomImageDataset(Dataset):\n","    def __init__(self, annotations_file, transform=None):\n","\n","        self.dataframe = annotations_file\n","        self.transform = transform\n","        \n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        # 重點，自寫讀檔案方法\n","        img_path = self.dataframe.iloc[idx, 0]        \n","        np_araay = np.transpose(np.load(img_path.replace('\\\\', '/')))\n","\n","        \n","        label = self.dataframe.iloc[idx, 1]\n","\n","        if self.transform:\n","            np_araay = self.transform(np_araay)\n","        \n","        \n","        return np_araay, label, img_path "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:52:42.113579Z","iopub.status.busy":"2023-11-02T14:52:42.112880Z","iopub.status.idle":"2023-11-02T14:52:42.123391Z","shell.execute_reply":"2023-11-02T14:52:42.122357Z","shell.execute_reply.started":"2023-11-02T14:52:42.113548Z"},"trusted":true},"outputs":[],"source":["# https://shenxiaohai.me/2018/10/19/pytorch_tutorial_intermediate_02/\n","class Network(nn.Module):\n","    def __init__(self, pool = 6, hid_lay=1, fc1= 256, outlay = 64):\n","        super(Network, self).__init__()\n","        \n","        self.cnn1 = nn.Conv1d(2, 64, 3, padding=1)\n","        self.cnn2 = nn.Conv1d(16, 32, 3, padding=1)\n","        self.maxpool1 = nn.MaxPool1d(32, 2)\n","        self.outlay = outlay\n","        self.LSTM1 = nn.LSTM(1251, self.outlay, hid_lay)#, bidirectional=True)\n","        \n","        self.pools = pool\n","        self.ave_pool = nn.AdaptiveAvgPool1d(self.pools)\n","        \n","        # FC\n","        \n","        self.fc1num = fc1\n","        self.fc1 = nn.Linear(2*self.outlay, self.fc1num)\n","        self.fc2 = nn.Linear(self.fc1num, 64)\n","        self.fc3 = nn.Linear(64, 1)\n","\n","        self.soft = nn.Sigmoid()\n","\n","        self.drop = nn.Dropout(0.3)\n","\n","\n","    def forward(self, input1):\n","        output = self.LSTM1(input1)[0]\n","\n","        output = output.view(-1, 2*self.outlay)\n","                \n","        con = self.fc1(output)\n","        con = F.relu(self.drop(con))\n","        con = self.fc2(con)   \n","        con = self.fc3(con)\n","        con = self.soft(con)\n","\n","        return con"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:52:42.617056Z","iopub.status.busy":"2023-11-02T14:52:42.616094Z","iopub.status.idle":"2023-11-02T14:52:42.623045Z","shell.execute_reply":"2023-11-02T14:52:42.621989Z","shell.execute_reply.started":"2023-11-02T14:52:42.617021Z"},"trusted":true},"outputs":[],"source":["# 分資料，使用StratifiedKFold\n","skf = StratifiedKFold(n_splits=5)\n","fold1 = list(skf.split(df_second, df_second['label']))"]},{"cell_type":"markdown","metadata":{},"source":["## **使用Optuna來找出最佳參數**\n","1. 建立 object_fun\n","2. 將預測試之變數用trial替換，如要測試categorical資料就用suggest_categorical，要測試"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:14:49.453775Z","iopub.status.busy":"2023-11-02T15:14:49.453396Z","iopub.status.idle":"2023-11-02T15:14:49.479083Z","shell.execute_reply":"2023-11-02T15:14:49.478036Z","shell.execute_reply.started":"2023-11-02T15:14:49.453743Z"},"trusted":true},"outputs":[],"source":["def object_fun(trial):\n","    higest_train = []\n","    higest_test = []\n","    for fold in fold1:   \n","        # 最高分\n","        higest_train_acc = 0\n","        higest_test_acc = 0.  \n","        \n","        # 存結果\n","        train_loss_list = []\n","        train_acc_list = []\n","        test_loss_list = []\n","        test_acc_list = []\n","        \n","        \n","        # 提出train_datasets與testing datasets\n","        X_df = df_second.loc[fold[0]]\n","        y_df = df_second.loc[fold[1]]\n","\n","        # 建造transform, \n","        trans_comp = transforms.Compose([transforms.ToTensor()])\n","\n","        # 建立dataset變套用transform\n","        train_dataset = CustomImageDataset(X_df, transform= trans_comp)\n","        test_dataset = CustomImageDataset(y_df, transform= trans_comp)\n","        print(train_dataset.__len__(), test_dataset.__len__())\n","\n","        # 把dataset放入dataloader\n","        train_dl = DataLoader(train_dataset, batch_size= 16, shuffle= False, drop_last=True)\n","        test_dl = DataLoader(test_dataset, batch_size= 16, shuffle= False)\n","        train_dl_eval = DataLoader(train_dataset, batch_size= 16, shuffle= False, drop_last=False)\n","        \n","        #  neural network model\n","        outlay = trial.suggest_categorical('outlay', [64, 96, 128])\n","        fc1 = trial.suggest_categorical('fc1', [256, 128, 64])\n","        \n","        model = Network(fc1= fc1, outlay = outlay).to(device)\n","\n","        criterion = nn.BCELoss()\n","\n","        # leaerning rate\n","        lr = trial.suggest_float('lr_rate', 0.001, 0.002)\n","        # optimizer\n","        optimizer = Adam(model.parameters(), lr=lr, weight_decay= 0.01)\n","\n","\n","\n","        # 幾個epoch\n","        epoch = 300\n","        \n","        \n","        for epoch in tqdm(range(epoch)):\n","            # 訓練訓練訓練訓練訓練訓練訓練訓練訓練訓練訓練  \n","            losses = 0.  #記得加 . 代表float\n","            accuracies = 0.\n","            total = 0\n","\n","            for batch, (X, y, zz) in enumerate(train_dl):\n","                # 每一個batch都要train\n","                model.train()\n","\n","                inputs, labels = X.squeeze().float().to(device), y.float().reshape([-1, 1]).to(device)\n","                optimizer.zero_grad()\n","                pred_out = model(inputs)\n","\n","                loss = criterion(pred_out, labels)\n","                losses = losses + loss.item()\n","\n","                # Backpropagation        \n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                digital = np.where(pred_out.to('cpu') < 0.5, 0, 1)\n","                labels = np.array(labels.to('cpu'), dtype= int)\n","\n","                accuracies += np.sum(digital == labels)\n","                total += len(labels)\n","        \n","            # 計算loss\n","            loss_ave = losses/(batch+1)\n","            acc_ave = accuracies/total\n","\n","            # 測試for training data\n","            model.eval()\n","            \n","            digital_list, label_list = [], []            \n","            losses1 = 0.\n","            for batch, (X, y, z) in enumerate(train_dl_eval):\n","            \n","                inputs, labels = X.squeeze().float().to(device), y.float().reshape([-1, 1]).to(device)\n","                pred_out = model(inputs)\n","\n","                loss1 = criterion(pred_out, labels)\n","\n","                # Backpropagation\n","                losses1 = losses1 + loss1.item()\n","\n","                digital1 = np.where(pred_out.to('cpu') < 0.5, 0, 1)\n","                labels1 = np.array(labels.to('cpu'), dtype= int)\n","\n","                digital_list += list(digital1.ravel())\n","                label_list += list(labels1.ravel())\n","\n","\n","\n","            loss_ave = losses1/(batch+1)\n","            acc_ave = accuracy_score(digital_list, label_list)\n","\n","            # 測試測試測試測試for testing data\n","            num_batches = len(test_dl)\n","            test_loss, test_accuracies, test_total = 0., 0., 0\n","            model.eval()\n","\n","            with torch.no_grad():\n","                for batch, (X, y, z) in enumerate(test_dl):\n","                    X = X.squeeze().float().to(device)\n","                    y = y.float().reshape([-1, 1]).to(device)\n","                    pred = model(X)\n","                    test_loss += criterion(pred, y).item()\n","                    digital = np.where(pred.to('cpu') < 0.5, 0, 1)\n","                    labels = np.array(y.to('cpu'), dtype= int)\n","\n","#                     test_accuracies += np.sum(digital == labels)\n","#                     test_total += len(labels)\n","\n","\n","            test_loss = test_loss/(batch+1)\n","            test_accuracies = accuracy_score(digital, labels)\n","\n","\n","            # append到list上面\n","            train_loss_list.append(loss_ave)\n","            train_acc_list.append(acc_ave)\n","            test_loss_list.append(test_loss)\n","            test_acc_list.append(test_accuracies)\n","            \n","            \n","            # 存Test best model\n","            if test_accuracies > higest_test_acc and acc_ave > test_accuracies  and acc_ave >= 0.8 and acc_ave <= 0.94:\n","                higest_test_acc = test_accuracies\n","                higest_train_acc = acc_ave\n","                print(higest_test_acc, higest_train_acc)\n","    \n","\n","        higest_train.append(higest_train_acc)\n","        higest_test.append(higest_test_acc)\n","        print(higest_train_acc, \" \", higest_test_acc)\n","\n","\n","\n","    print(np.mean(higest_train))\n","    print(np.mean(higest_test))\n","    print(\"Data Length\", len(higest_train))\n","\n","    return np.mean(higest_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:14:50.570458Z","iopub.status.busy":"2023-11-02T15:14:50.570072Z","iopub.status.idle":"2023-11-02T15:14:50.577221Z","shell.execute_reply":"2023-11-02T15:14:50.575633Z","shell.execute_reply.started":"2023-11-02T15:14:50.570427Z"},"trusted":true},"outputs":[],"source":["# define sample\n","sampler = optuna.samplers.TPESampler(seed=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["try:\n","    study = optuna.create_study(storage=\"sqlite:///cnn_npy_52_channel.db\", study_name=\"mystudy1\", direction='maximize', sampler=sampler)\n","except:\n","    study = optuna.load_study(study_name=\"mystudy0\", storage=\"sqlite:///cnn_npy_52_channel.db\")\n","    \n","study.optimize(object_fun, n_trials=1000)"]},{"cell_type":"markdown","metadata":{},"source":["## **選出最好的參數並跑一次**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:29:15.244965Z","iopub.status.busy":"2023-11-02T15:29:15.244570Z","iopub.status.idle":"2023-11-02T15:29:15.249593Z","shell.execute_reply":"2023-11-02T15:29:15.248674Z","shell.execute_reply.started":"2023-11-02T15:29:15.244934Z"},"trusted":true},"outputs":[],"source":["good_param = {'fc1': 128, 'lr_rate': 0.0010904593492709073, 'outlay': 128}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:37:28.591293Z","iopub.status.busy":"2023-11-02T15:37:28.590548Z","iopub.status.idle":"2023-11-02T15:41:39.667772Z","shell.execute_reply":"2023-11-02T15:41:39.666773Z","shell.execute_reply.started":"2023-11-02T15:37:28.591259Z"},"trusted":true},"outputs":[],"source":["df_one = pd.DataFrame()\n","\n","higest_train = []\n","higest_test = []\n","for kk, fold in enumerate(fold1):   \n","    higest_train_acc = 0\n","    higest_test_acc = 0\n","\n","\n","    X_df = df_second.loc[fold[0]]\n","    y_df = df_second.loc[fold[1]]\n","    X_df, y_df\n","\n","    # 建造transform, \n","    trans_comp = transforms.Compose([transforms.ToTensor()])  # transforms.Grayscale(), transforms.PILToTensor(), \n","\n","    # 建立dataset變套用transform\n","    train_dataset = CustomImageDataset(X_df, transform= trans_comp)\n","    test_dataset = CustomImageDataset(y_df, transform= trans_comp)\n","\n","    print(train_dataset.__len__(), test_dataset.__len__())\n","\n","    # 把dataset放入dataloader\n","    train_dl = DataLoader(train_dataset, batch_size= 16, shuffle= False, drop_last=True)\n","    test_dl = DataLoader(test_dataset, batch_size= 16, shuffle= False)\n","    \n","    train_dl_eval = DataLoader(train_dataset, batch_size= 16, shuffle= False, drop_last=False)\n","\n","    #  neural network model 放到GPU\n","     \n","    outlay = good_param['outlay']\n","    fc1 = good_param['fc1']\n","    \n","    lr = good_param['lr_rate']\n","\n","    model = Network(fc1= fc1, outlay = outlay).to(device)\n","    \n","    criterion = nn.BCELoss()\n","\n","    # SGD\n","    optimizer = Adam(model.parameters(), lr=lr, weight_decay= 0.01)  # lr=0.0098632002427510 過小\n","\n","    # 模型存下來要叫啥\n","    name = 'Test_minmax_task_m_rest_as_bpd_npy'\n","\n","    # 存結果\n","    train_loss_list = []\n","    train_acc_list = []\n","    test_loss_list = []\n","    test_acc_list = []\n","    higest_test_acc = 0.\n","\n","    # 存取learning rate\n","    cur_lr_list = []\n","\n","    # 幾個epoch\n","    # 幾個epoch\n","    epoch = 300\n","\n","\n","    for epoch in tqdm(range(epoch)):\n","        # 訓練訓練訓練訓練訓練訓練訓練訓練訓練訓練訓練  \n","        losses = 0.  #記得加 . 代表float\n","        accuracies = 0.\n","        total = 0\n","\n","        for batch, (X, y, zz) in enumerate(train_dl):\n","            # 每一個batch都要train\n","            model.train()\n","\n","            inputs, labels = X.squeeze().float().to(device), y.float().reshape([-1, 1]).to(device)\n","            optimizer.zero_grad()\n","            pred_out = model(inputs)\n","\n","            loss = criterion(pred_out, labels)\n","            losses = losses + loss.item()\n","\n","            # Backpropagation        \n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            digital = np.where(pred_out.to('cpu') < 0.5, 0, 1)\n","            labels = np.array(labels.to('cpu'), dtype= int)\n","\n","            accuracies += np.sum(digital == labels)\n","            total += len(labels)\n","\n","        # 計算loss\n","        loss_ave = losses/(batch+1)\n","        acc_ave = accuracies/total\n","\n","        # 測試for training data\n","        model.eval()\n","\n","        digital_list, label_list = [], []            \n","        losses1 = 0.\n","        for batch, (X, y, z) in enumerate(train_dl_eval):\n","\n","            inputs, labels = X.squeeze().float().to(device), y.float().reshape([-1, 1]).to(device)\n","            pred_out = model(inputs)\n","\n","            loss1 = criterion(pred_out, labels)\n","\n","            # Backpropagation\n","            losses1 = losses1 + loss1.item()\n","\n","            digital1 = np.where(pred_out.to('cpu') < 0.5, 0, 1)\n","            labels1 = np.array(labels.to('cpu'), dtype= int)\n","\n","            digital_list += list(digital1.ravel())\n","            label_list += list(labels1.ravel())\n","\n","\n","\n","        loss_ave = losses1/(batch+1)\n","        acc_ave = accuracy_score(digital_list, label_list)\n","\n","        # 測試測試測試測試for testing data\n","        num_batches = len(test_dl)\n","        test_loss, test_accuracies, test_total = 0., 0., 0\n","        model.eval()\n","\n","        with torch.no_grad():\n","            for batch, (X, y, z) in enumerate(test_dl):\n","                X = X.squeeze().float().to(device)\n","                y = y.float().reshape([-1, 1]).to(device)\n","                pred = model(X)\n","                test_loss += criterion(pred, y).item()\n","                digital = np.where(pred.to('cpu') < 0.5, 0, 1)\n","                labels = np.array(y.to('cpu'), dtype= int)\n","\n","\n","        test_loss = test_loss/(batch+1)\n","        test_accuracies = accuracy_score(digital, labels)\n","\n","\n","        # append到list上面\n","        train_loss_list.append(loss_ave)\n","        train_acc_list.append(acc_ave)\n","        test_loss_list.append(test_loss)\n","        test_acc_list.append(test_accuracies)\n","\n","\n","        # test acc大且 train acc大於test acc，則存模型，原本如下\n","\n","        if test_accuracies > higest_test_acc and acc_ave > test_accuracies and acc_ave >= 0.76 and acc_ave <= 0.96: #and f1_score(labels, digital) > 0.8 \n","            higest_test_acc = test_accuracies\n","            higest_train_acc = acc_ave\n","            torch.save(model.state_dict(), \"{}_{}.pth\".format(kk, name+\"_epoch_\" +str(epoch)))\n","            print('F1_score= ', f1_score(labels, digital))\n","            print('File: ', \"{}_{}.pth\".format(kk, name+\"_epoch_\" +str(epoch)))     \n","                \n","    print('\\n\\n')\n","    print(\"Good train:\", higest_train_acc)\n","    print(\"Good test:\", higest_test_acc)\n","            \n","\n","    higest_train.append(higest_train_acc)\n","    higest_test.append(higest_test_acc)\n","\n","    \n","    df_one[f'train_loss_{kk}'] = train_loss_list\n","    df_one[f'train_acc_{kk}'] = train_acc_list\n","    df_one[f'test_acc_{kk}'] = test_loss_list\n","    df_one[f'test_acc_{kk}'] = test_acc_list\n","\n","\n","\n","print('Mean of train: ', np.mean(higest_train),\", Mean of test\", np.mean(higest_test))\n","print(\"Data Length\", len(higest_train))"]},{"cell_type":"markdown","metadata":{},"source":["## **畫出訓練結果**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:48:36.090917Z","iopub.status.busy":"2023-11-02T15:48:36.090538Z","iopub.status.idle":"2023-11-02T15:48:36.098559Z","shell.execute_reply":"2023-11-02T15:48:36.097419Z","shell.execute_reply.started":"2023-11-02T15:48:36.090888Z"},"trusted":true},"outputs":[],"source":["df_one.loc[0] = 0\n","df_one = df_one.sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:48:37.141323Z","iopub.status.busy":"2023-11-02T15:48:37.140511Z","iopub.status.idle":"2023-11-02T15:48:38.853088Z","shell.execute_reply":"2023-11-02T15:48:38.852186Z","shell.execute_reply.started":"2023-11-02T15:48:37.141288Z"},"trusted":true},"outputs":[],"source":["for i in range(5):\n","    plt.figure(figsize= (16,8))\n","    plt.plot(df_one.index, df_one[f'train_acc_{i}'], label= 'train_acc', linewidth=3)\n","    plt.plot(df_one.index, df_one[f'test_acc_{i}'], label= 'test_acc', linewidth=3)\n","    plt.legend(fontsize= 15 )\n","    plt.xlabel('Epochs', fontsize= 15)\n","    plt.ylabel('Accuracy', fontsize= 15)\n","    plt.title('Control vs Schizophrenia', fontsize= 15)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## **讀取最好的模型並重現預測**\n","1. 讀取模型\n","2. 重現testing data 並畫confusion matrix\n","3. 重現training data 並畫confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:49:39.420937Z","iopub.status.busy":"2023-11-02T15:49:39.420255Z","iopub.status.idle":"2023-11-02T15:49:39.430158Z","shell.execute_reply":"2023-11-02T15:49:39.429160Z","shell.execute_reply.started":"2023-11-02T15:49:39.420902Z"},"trusted":true},"outputs":[],"source":["higest_train_acc = 0\n","higest_test_acc = 0\n","\n","\n","X_df = df_second.loc[fold1[3][0]]\n","y_df = df_second.loc[fold1[3][1]]\n","X_df, y_df\n","\n","# 建造transform, \n","trans_comp = transforms.Compose([transforms.ToTensor()])  # transforms.Grayscale(), transforms.PILToTensor(), \n","\n","# 建立dataset變套用transform\n","train_dataset = CustomImageDataset(X_df, transform= trans_comp)\n","test_dataset = CustomImageDataset(y_df, transform= trans_comp)\n","\n","print(train_dataset.__len__(), test_dataset.__len__())\n","\n","# 把dataset放入dataloader\n","train_dl = DataLoader(train_dataset, batch_size= 16, shuffle= False, drop_last=False)\n","test_dl = DataLoader(test_dataset, batch_size= 16, shuffle= False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:51:57.729396Z","iopub.status.busy":"2023-11-02T15:51:57.728346Z","iopub.status.idle":"2023-11-02T15:51:57.740742Z","shell.execute_reply":"2023-11-02T15:51:57.739746Z","shell.execute_reply.started":"2023-11-02T15:51:57.729358Z"},"trusted":true},"outputs":[],"source":["# Load model\n","model.load_state_dict(torch.load('/kaggle/working/{}_{}.pth'.format(3, name+\"_epoch_\" +str(129))))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:55:46.601268Z","iopub.status.busy":"2023-11-02T15:55:46.600436Z","iopub.status.idle":"2023-11-02T15:55:46.635208Z","shell.execute_reply":"2023-11-02T15:55:46.634001Z","shell.execute_reply.started":"2023-11-02T15:55:46.601223Z"},"trusted":true},"outputs":[],"source":["# test\n","size = len(test_dl.dataset)\n","num_batches = len(test_dl)\n","test_loss, test_accuracies, total_len = 0, 0, 0\n","digital_list = []\n","label_list = []\n","model.eval()\n","with torch.no_grad():\n","    for batch, (X, y, z) in enumerate(test_dl):\n","        X = X.squeeze().float().to(device)\n","        y = y.float().reshape([-1, 1]).to(device)\n","        pred = model(X)\n","        test_loss += criterion(pred, y).item()\n","        digital = np.where(pred.to('cpu') < 0.5, 0, 1)\n","        labels = np.array(y.to('cpu'), dtype= int)\n","        digital_list += list(digital)\n","        label_list += list(labels)\n","\n","test_loss = test_loss/(batch+1)\n","test_accuracies = accuracy_score(digital, labels)\n","print(f\"Test Error: \\n Accuracy: {(100*test_accuracies):5f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:56:32.661767Z","iopub.status.busy":"2023-11-02T15:56:32.661399Z","iopub.status.idle":"2023-11-02T15:56:32.965100Z","shell.execute_reply":"2023-11-02T15:56:32.964264Z","shell.execute_reply.started":"2023-11-02T15:56:32.661734Z"},"trusted":true},"outputs":[],"source":["cm = confusion_matrix(np.array(label_list).squeeze(), np.array(digital_list).squeeze())\n","plt.figure(figsize = (8, 6))\n","sns.heatmap(cm, annot = True, fmt='.20g', cmap= sns.color_palette(\"Blues\", as_cmap=True), xticklabels= ['Schizophrenia', 'Control'], yticklabels= ['Schizophrenia', 'Control'], annot_kws={\"size\": 16})\n","plt.ylabel('Actual', fontsize=15)\n","plt.xlabel('Predicted', fontsize=15)\n","plt.title('Test Confusion Matrix')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:56:36.101496Z","iopub.status.busy":"2023-11-02T15:56:36.100805Z","iopub.status.idle":"2023-11-02T15:56:36.170042Z","shell.execute_reply":"2023-11-02T15:56:36.169125Z","shell.execute_reply.started":"2023-11-02T15:56:36.101462Z"},"trusted":true},"outputs":[],"source":["# train \n","size = len(train_dl.dataset)    \n","print(size)\n","losses = 0.  \n","accuracies = 0.\n","total_len_t = 0\n","digital_list = []\n","label_list = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for batch, (X, y, z) in enumerate(train_dl):\n","        # Compute prediction and loss\n","        print(batch, \"~~~~~\", end= '\\r')\n","\n","        inputs, labels = X.squeeze().float().to(device), y.float().reshape([-1, 1]).to(device)\n","\n","        pred_out = model(inputs)\n","        \n","        loss = criterion(pred_out, labels)\n","\n","        # Backpropagation\n","        losses = losses + loss.item()\n","\n","        digital = np.where(pred_out.to('cpu') < 0.5, 0, 1)\n","        labels = np.array(labels.to('cpu'), dtype= int)\n","\n","        digital_list += list(digital.ravel())\n","        label_list += list(labels.ravel())\n","        \n","\n","        accuracies += np.sum(digital == labels)\n","        total_len_t += len(labels)\n","\n","loss_ave = losses/(batch+1)\n","acc_ave = accuracies/total_len_t\n","\n","print(f\"loss: {loss_ave} accuracy: {acc_ave}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T15:57:00.692080Z","iopub.status.busy":"2023-11-02T15:57:00.691330Z","iopub.status.idle":"2023-11-02T15:57:00.932633Z","shell.execute_reply":"2023-11-02T15:57:00.931730Z","shell.execute_reply.started":"2023-11-02T15:57:00.692045Z"},"trusted":true},"outputs":[],"source":["cm = confusion_matrix(label_list, digital_list)\n","plt.figure(figsize = (8, 6))\n","sns.heatmap(cm, annot = True, fmt='.20g', cmap= sns.color_palette(\"Blues\", as_cmap=True), xticklabels= ['Schizophrenia', 'Control'], yticklabels= ['Schizophrenia', 'Control'], annot_kws={\"size\": 16})\n","plt.ylabel('Actual', fontsize=15)\n","plt.xlabel('Predicted', fontsize=15)\n","plt.title('Train Confusion Matrix')"]},{"cell_type":"markdown","metadata":{},"source":["# **可視化AI - Intergated Gradients **\n","> https://captum.ai/docs/extension/integrated_gradients\n","\n","1. 使用Captum的IntegratedGradients，將模型與訊號丟入之後得出attributions，及模型所關注的區域\n","2. 使用rolling apply數值平滑，以方便畫圖\n","3. 在背景畫出模型主要所看的位置"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:03:13.333420Z","iopub.status.busy":"2023-11-02T16:03:13.333036Z","iopub.status.idle":"2023-11-02T16:03:13.338711Z","shell.execute_reply":"2023-11-02T16:03:13.337648Z","shell.execute_reply.started":"2023-11-02T16:03:13.333391Z"},"trusted":true},"outputs":[],"source":["def minmax(x):\n","    return (x - x.min())/(x.max() - x.min())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:08:31.782863Z","iopub.status.busy":"2023-11-02T16:08:31.782533Z","iopub.status.idle":"2023-11-02T16:12:14.686977Z","shell.execute_reply":"2023-11-02T16:12:14.686040Z","shell.execute_reply.started":"2023-11-02T16:08:31.782834Z"},"trusted":true},"outputs":[],"source":["model.train()\n","ig = IntegratedGradients(model)\n","for batch, (X, y, zz) in enumerate(train_dl):\n","    \n","    df_cuba = pd.DataFrame()\n","    input1 = X.squeeze().float().to(device)\n","    # 把處理過的.npy改成原始.npy\n","    z = [x.replace('region_h', 'nominmax_h').replace('region_un', 'nominmax_un').replace('syn-time-series', 'nominmax').replace('.npy', '_nominmax.npy') for x in zz]\n","    \n","    # 製作IntegratedGradients\n","    attributions, delta = ig.attribute(input1, target=0, return_convergence_delta=True)\n","    for nums, att in enumerate(attributions):\n","        df_cuba[f'{nums}_Region_0'] = att.cpu()[0]\n","        df_cuba[f'{nums}_Region_1'] = att.cpu()[1]\n","\n","    df_cleann = minmax(df_cuba.rolling(50).mean().bfill())\n","\n","    \n","    for ind in range(len(attributions)):\n","        # First plot\n","        y = np.linspace(-0.6, 1, 100)\n","        cmapp1 = [(1, 0, 0, a*0.5) for a in df_cleann[f'{ind}_Region_0']]    \n","        # 繪製水平色彩漸進圖\n","        fig, ax = plt.subplots(figsize=(40, 15))\n","\n","        for i in range(len(colors)):\n","            ax.plot(np.ones_like(y) * i/10, y, color=cmapp2[i], linewidth=4)\n","\n","        datasss = np.transpose(np.load(z[ind]),(1,0))\n","        plt.plot(np.arange(0, len(datasss[0])/10, 0.1), datasss[0], label= 'region1', c='black', linewidth=10)\n","        plt.xticks(fontsize=20)\n","        plt.yticks(fontsize=20)\n","        plt.xlabel('Second(s)', fontsize=30)\n","\n","        plt.xlim(-0.1, 125.1)\n","        plt.ylim(-0.6, 1)\n","        plt.title(f'{z[ind]}_Region 1', fontsize=30)\n","\n","        # Second plot\n","        y = np.linspace(-0.6, 1, 100)\n","\n","        cmapp2 = [(1, 0, 0, a*0.5) for a in df_cleann[f'{ind}_Region_1']]\n","\n","        # 繪製水平色彩漸進圖\n","        fig, ax = plt.subplots(figsize=(40, 15))\n","\n","        for i in range(len(colors)):\n","            ax.plot(np.ones_like(y) * i/10, y, color=cmapp2[i], linewidth=4)\n","\n","        plt.plot(np.arange(0, len(datasss[1])/10, 0.1), datasss[1], label= 'region1', c='black', linewidth=10)\n","        plt.xlim(-0.1, 125.1)\n","        plt.ylim(-0.6, 1)\n","        plt.title(f'{z[ind]}_Region 2', fontsize=30)\n","        plt.xticks(fontsize=20)\n","        plt.yticks(fontsize=20)\n","        plt.xlabel('Second(s)', fontsize=30)\n","        plt.show()\n","        print(\"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
